{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10962096,"sourceType":"datasetVersion","datasetId":6820026}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import save_image\nimport os\nimport random\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport argparse\n\n# Set random seeds for reproducibility\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seed(42)\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Custom Dataset for paired and unpaired sketch-image data\nclass SketchImageDataset(Dataset):\n    def __init__(self, sketch_dir, image_dir, transform=None):\n        \"\"\"\n        Dataset for sketch to image translation\n        \n        Args:\n            sketch_dir: Directory containing sketch images\n            image_dir: Directory containing corresponding real images\n            transform: Optional transforms to apply\n        \"\"\"\n        self.sketch_dir = sketch_dir\n        self.image_dir = image_dir\n        self.transform = transform\n        \n        self.sketch_files = sorted(os.listdir(sketch_dir))\n        self.image_files = sorted(os.listdir(image_dir))\n        \n    def __len__(self):\n        return len(self.sketch_files)\n    \n    def __getitem__(self, idx):\n        sketch_path = os.path.join(self.sketch_dir, self.sketch_files[idx])\n        # For CycleGAN style training, get a random real image\n        random_idx = random.randint(0, len(self.image_files) - 1)\n        image_path = os.path.join(self.image_dir, self.image_files[random_idx])\n        \n        sketch = Image.open(sketch_path).convert('RGB')\n        real_image = Image.open(image_path).convert('RGB')\n        \n        if self.transform:\n            sketch = self.transform(sketch)\n            real_image = self.transform(real_image)\n            \n        return sketch, real_image\n\n# DCGAN Generator with UNet-style skip connections\nclass Generator(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3, ngf=64):\n        \"\"\"\n        Generator architecture combining DCGAN structure with UNet skip connections\n        \n        Args:\n            in_channels: Number of input channels (3 for RGB)\n            out_channels: Number of output channels (3 for RGB)\n            ngf: Number of generator filters\n        \"\"\"\n        super(Generator, self).__init__()\n        \n        # Encoder (downsampling)\n        self.down1 = nn.Sequential(\n            nn.Conv2d(in_channels, ngf, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2, inplace=True)\n        )  # 128x128 -> 64x64\n        \n        self.down2 = nn.Sequential(\n            nn.Conv2d(ngf, ngf * 2, kernel_size=4, stride=2, padding=1),\n            nn.InstanceNorm2d(ngf * 2),\n            nn.LeakyReLU(0.2, inplace=True)\n        )  # 64x64 -> 32x32\n        \n        self.down3 = nn.Sequential(\n            nn.Conv2d(ngf * 2, ngf * 4, kernel_size=4, stride=2, padding=1),\n            nn.InstanceNorm2d(ngf * 4),\n            nn.LeakyReLU(0.2, inplace=True)\n        )  # 32x32 -> 16x16\n        \n        self.down4 = nn.Sequential(\n            nn.Conv2d(ngf * 4, ngf * 8, kernel_size=4, stride=2, padding=1),\n            nn.InstanceNorm2d(ngf * 8),\n            nn.LeakyReLU(0.2, inplace=True)\n        )  # 16x16 -> 8x8\n        \n        # Bottleneck (residual blocks)\n        self.res_blocks = nn.Sequential(\n            ResidualBlock(ngf * 8),\n            ResidualBlock(ngf * 8),\n            ResidualBlock(ngf * 8),\n            ResidualBlock(ngf * 8),\n            ResidualBlock(ngf * 8)\n        )\n        \n        # Decoder (upsampling)\n        self.up1 = nn.Sequential(\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size=4, stride=2, padding=1),\n            nn.InstanceNorm2d(ngf * 4),\n            nn.ReLU(inplace=True)\n        )  # 8x8 -> 16x16\n        \n        self.up2 = nn.Sequential(\n            nn.ConvTranspose2d(ngf * 8, ngf * 2, kernel_size=4, stride=2, padding=1),\n            nn.InstanceNorm2d(ngf * 2),\n            nn.ReLU(inplace=True)\n        )  # 16x16 -> 32x32\n        \n        self.up3 = nn.Sequential(\n            nn.ConvTranspose2d(ngf * 4, ngf, kernel_size=4, stride=2, padding=1),\n            nn.InstanceNorm2d(ngf),\n            nn.ReLU(inplace=True)\n        )  # 32x32 -> 64x64\n        \n        self.up4 = nn.Sequential(\n            nn.ConvTranspose2d(ngf * 2, out_channels, kernel_size=4, stride=2, padding=1),\n            nn.Tanh()\n        )  # 64x64 -> 128x128\n        \n    def forward(self, x):\n        # Encoder\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        \n        # Bottleneck\n        out = self.res_blocks(d4)\n        \n        # Decoder with skip connections (UNet style)\n        u1 = self.up1(out)\n        u1 = torch.cat([u1, d3], dim=1)  # Skip connection\n        \n        u2 = self.up2(u1)\n        u2 = torch.cat([u2, d2], dim=1)  # Skip connection\n        \n        u3 = self.up3(u2)\n        u3 = torch.cat([u3, d1], dim=1)  # Skip connection\n        \n        return self.up4(u3)\n\n# Residual Block for Generator\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(channels, channels, kernel_size=3, padding=0),\n            nn.InstanceNorm2d(channels),\n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(channels, channels, kernel_size=3, padding=0),\n            nn.InstanceNorm2d(channels)\n        )\n    \n    def forward(self, x):\n        return x + self.block(x)  # Skip connection\n\n# Discriminator based on DCGAN\nclass Discriminator(nn.Module):\n    def __init__(self, in_channels=3, ndf=64):\n        \"\"\"\n        PatchGAN discriminator architecture from CycleGAN\n        \n        Args:\n            in_channels: Number of input channels (3 for RGB)\n            ndf: Number of discriminator filters\n        \"\"\"\n        super(Discriminator, self).__init__()\n        \n        # Input: 128x128x3\n        self.model = nn.Sequential(\n            # Layer 1: 128x128 -> 64x64\n            nn.Conv2d(in_channels, ndf, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Layer 2: 64x64 -> 32x32\n            nn.Conv2d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1),\n            nn.InstanceNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Layer 3: 32x32 -> 16x16\n            nn.Conv2d(ndf * 2, ndf * 4, kernel_size=4, stride=2, padding=1),\n            nn.InstanceNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Layer 4: 16x16 -> 8x8\n            nn.Conv2d(ndf * 4, ndf * 8, kernel_size=4, stride=2, padding=1),\n            nn.InstanceNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Output layer: 8x8 -> 7x7 (PatchGAN)\n            nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=1, padding=1)\n        )\n    \n    def forward(self, x):\n        return self.model(x)\n\n# Replay Buffer for CycleGAN's improved stability\nclass ReplayBuffer:\n    def __init__(self, max_size=50):\n        self.max_size = max_size\n        self.buffer = []\n    \n    def push_and_pop(self, data):\n        result = []\n        for element in data:\n            element = torch.unsqueeze(element, 0)\n            if len(self.buffer) < self.max_size:\n                self.buffer.append(element)\n                result.append(element)\n            else:\n                if random.random() < 0.5:\n                    i = random.randint(0, self.max_size - 1)\n                    result.append(self.buffer[i].clone())\n                    self.buffer[i] = element\n                else:\n                    result.append(element)\n        return torch.cat(result)\n\n# Training function for the hybrid GAN model\ndef train_hybrid_gan(sketch_dir, real_dir, epochs, batch_size, lr, beta1, beta2, lambda_cycle, \n                    lambda_identity, save_dir, sample_interval):\n    \"\"\"\n    Train the hybrid GAN model\n    \n    Args:\n        sketch_dir: Directory containing sketch images\n        real_dir: Directory containing real images\n        epochs: Number of training epochs\n        batch_size: Batch size\n        lr: Learning rate\n        beta1, beta2: Adam optimizer parameters\n        lambda_cycle: Weight for cycle consistency loss\n        lambda_identity: Weight for identity loss\n        save_dir: Directory to save models and samples\n        sample_interval: Interval for saving samples\n    \"\"\"\n    # Create directories\n    os.makedirs(save_dir, exist_ok=True)\n    os.makedirs(os.path.join(save_dir, 'samples'), exist_ok=True)\n    os.makedirs(os.path.join(save_dir, 'checkpoints'), exist_ok=True)\n    \n    # Transform for input images - resize to 128x128 and normalize\n    transform = transforms.Compose([\n        transforms.Resize((128, 128)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    \n    # Load dataset\n    dataset = SketchImageDataset(sketch_dir, real_dir, transform)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    \n    # Initialize models\n    G_sketch_to_real = Generator().to(device)\n    G_real_to_sketch = Generator().to(device)\n    D_real = Discriminator().to(device)\n    D_sketch = Discriminator().to(device)\n    \n    # Initialize replay buffers for CycleGAN stability\n    fake_real_buffer = ReplayBuffer()\n    fake_sketch_buffer = ReplayBuffer()\n    \n    # Loss functions\n    criterion_GAN = nn.MSELoss()  # For adversarial loss\n    criterion_cycle = nn.L1Loss()  # For cycle consistency loss\n    criterion_identity = nn.L1Loss()  # For identity loss\n    \n    # Optimizers\n    optimizer_G = optim.Adam(\n        list(G_sketch_to_real.parameters()) + list(G_real_to_sketch.parameters()),\n        lr=lr, betas=(beta1, beta2)\n    )\n    optimizer_D_real = optim.Adam(D_real.parameters(), lr=lr, betas=(beta1, beta2))\n    optimizer_D_sketch = optim.Adam(D_sketch.parameters(), lr=lr, betas=(beta1, beta2))\n    \n    # Learning rate scheduler\n    lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n        optimizer_G, lr_lambda=lambda epoch: 1.0 - max(0, epoch - epochs * 0.5) / (epochs * 0.5)\n    )\n    lr_scheduler_D_real = torch.optim.lr_scheduler.LambdaLR(\n        optimizer_D_real, lr_lambda=lambda epoch: 1.0 - max(0, epoch - epochs * 0.5) / (epochs * 0.5)\n    )\n    lr_scheduler_D_sketch = torch.optim.lr_scheduler.LambdaLR(\n        optimizer_D_sketch, lr_lambda=lambda epoch: 1.0 - max(0, epoch - epochs * 0.5) / (epochs * 0.5)\n    )\n    \n    # Training loop\n    for epoch in range(epochs):\n        pbar = tqdm(enumerate(dataloader), total=len(dataloader))\n        for i, (sketches, real_images) in pbar:\n            # Move data to device\n            real_images = real_images.to(device)\n            sketches = sketches.to(device)\n            \n            # Ground truths\n            valid = torch.ones((real_images.size(0), 1, 7, 7), device=device)\n            fake = torch.zeros((real_images.size(0), 1, 7, 7), device=device)\n            \n            # ------------------\n            # Train Generators\n            # ------------------\n            optimizer_G.zero_grad()\n            \n            # Identity loss\n            if lambda_identity > 0:\n                # G_sketch_to_real should generate the same real image when fed a real image\n                identity_real = G_sketch_to_real(real_images)\n                loss_identity_real = criterion_identity(identity_real, real_images) * lambda_identity\n                \n                # G_real_to_sketch should generate the same sketch when fed a sketch\n                identity_sketch = G_real_to_sketch(sketches)\n                loss_identity_sketch = criterion_identity(identity_sketch, sketches) * lambda_identity\n            else:\n                loss_identity_real = 0\n                loss_identity_sketch = 0\n            \n            # GAN loss for G_sketch_to_real\n            fake_real = G_sketch_to_real(sketches)\n            loss_GAN_sketch_to_real = criterion_GAN(D_real(fake_real), valid)\n            \n            # GAN loss for G_real_to_sketch\n            fake_sketch = G_real_to_sketch(real_images)\n            loss_GAN_real_to_sketch = criterion_GAN(D_sketch(fake_sketch), valid)\n            \n            # Cycle consistency loss\n            recovered_sketch = G_real_to_sketch(fake_real)\n            loss_cycle_sketch = criterion_cycle(recovered_sketch, sketches) * lambda_cycle\n            \n            recovered_real = G_sketch_to_real(fake_sketch)\n            loss_cycle_real = criterion_cycle(recovered_real, real_images) * lambda_cycle\n            \n            # Total generator loss\n            loss_G = (loss_identity_real + loss_identity_sketch + \n                     loss_GAN_sketch_to_real + loss_GAN_real_to_sketch + \n                     loss_cycle_sketch + loss_cycle_real)\n            \n            loss_G.backward()\n            optimizer_G.step()\n            \n            # -----------------------\n            # Train Discriminator Real\n            # -----------------------\n            optimizer_D_real.zero_grad()\n            \n            # Real loss\n            loss_real_real = criterion_GAN(D_real(real_images), valid)\n            \n            # Fake loss (with buffer)\n            fake_real_ = fake_real_buffer.push_and_pop(fake_real.detach())\n            loss_fake_real = criterion_GAN(D_real(fake_real_), fake)\n            \n            # Total discriminator real loss\n            loss_D_real = (loss_real_real + loss_fake_real) * 0.5\n            loss_D_real.backward()\n            optimizer_D_real.step()\n            \n            # -----------------------\n            # Train Discriminator Sketch\n            # -----------------------\n            optimizer_D_sketch.zero_grad()\n            \n            # Real loss\n            loss_real_sketch = criterion_GAN(D_sketch(sketches), valid)\n            \n            # Fake loss (with buffer)\n            fake_sketch_ = fake_sketch_buffer.push_and_pop(fake_sketch.detach())\n            loss_fake_sketch = criterion_GAN(D_sketch(fake_sketch_), fake)\n            \n            # Total discriminator sketch loss\n            loss_D_sketch = (loss_real_sketch + loss_fake_sketch) * 0.5\n            loss_D_sketch.backward()\n            optimizer_D_sketch.step()\n            \n            # Update progress bar\n            pbar.set_description(\n                f\"[Epoch {epoch+1}/{epochs}] \"\n                f\"D_real: {loss_D_real.item():.4f}, D_sketch: {loss_D_sketch.item():.4f}, \"\n                f\"G: {loss_G.item():.4f}, G_adv: {(loss_GAN_sketch_to_real + loss_GAN_real_to_sketch).item():.4f}, \"\n                f\"G_cycle: {(loss_cycle_sketch + loss_cycle_real).item():.4f}\"\n            )\n            \n            # Save sample images\n            if i % sample_interval == 0:\n                batch_size = real_images.size(0)\n                # Take up to 8 images for visualization\n                n_samples = min(8, batch_size)\n                \n                # Generate fake images\n                fake_real_samples = fake_real.detach()[:n_samples]\n                fake_sketch_samples = fake_sketch.detach()[:n_samples]\n                recovered_real_samples = recovered_real.detach()[:n_samples]\n                recovered_sketch_samples = recovered_sketch.detach()[:n_samples]\n                \n                # Original inputs\n                real_samples = real_images[:n_samples]\n                sketch_samples = sketches[:n_samples]\n                \n                # Combine all images\n                all_samples = torch.cat([\n                    sketch_samples, fake_real_samples, recovered_sketch_samples,\n                    real_samples, fake_sketch_samples, recovered_real_samples\n                ], dim=0)\n                \n                # Save grid image\n                grid = torchvision.utils.make_grid(all_samples, nrow=n_samples, normalize=True)\n                save_path = os.path.join(save_dir, 'samples', f'epoch_{epoch+1}_batch_{i}.png')\n                save_image(grid, save_path)\n                \n        # Update learning rates\n        lr_scheduler_G.step()\n        lr_scheduler_D_real.step()\n        lr_scheduler_D_sketch.step()\n        \n        # Save models\n        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n            torch.save({\n                'G_sketch_to_real': G_sketch_to_real.state_dict(),\n                'G_real_to_sketch': G_real_to_sketch.state_dict(),\n                'D_real': D_real.state_dict(),\n                'D_sketch': D_sketch.state_dict(),\n                'optimizer_G': optimizer_G.state_dict(),\n                'optimizer_D_real': optimizer_D_real.state_dict(),\n                'optimizer_D_sketch': optimizer_D_sketch.state_dict(),\n                'epoch': epoch\n            }, os.path.join(save_dir, 'checkpoints', f'model_epoch_{epoch+1}.pth'))\n\n# Function to generate images from sketches using a trained model\ndef generate_from_sketch(model_path, sketch_path, output_dir, num_samples=5):\n    \"\"\"\n    Generate realistic images from sketches using a trained model\n    \n    Args:\n        model_path: Path to trained model checkpoint\n        sketch_path: Path to sketch image or directory of sketches\n        output_dir: Directory to save generated images\n        num_samples: Number of samples to generate (with small variations)\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Load model\n    checkpoint = torch.load(model_path, map_location=device)\n    G_sketch_to_real = Generator().to(device)\n    G_sketch_to_real.load_state_dict(checkpoint['G_sketch_to_real'])\n    G_sketch_to_real.eval()\n    \n    # Transform for input images\n    transform = transforms.Compose([\n        transforms.Resize((128, 128)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    \n    # Check if sketch_path is a directory or a single file\n    if os.path.isdir(sketch_path):\n        sketch_files = [os.path.join(sketch_path, f) for f in os.listdir(sketch_path) \n                       if f.endswith(('.png', '.jpg', '.jpeg'))]\n    else:\n        sketch_files = [sketch_path]\n    \n    for sketch_file in sketch_files:\n        # Load and preprocess sketch\n        sketch = Image.open(sketch_file).convert('RGB')\n        sketch_tensor = transform(sketch).unsqueeze(0).to(device)\n        \n        # Generate multiple samples with small variations\n        for i in range(num_samples):\n            with torch.no_grad():\n                # Add small noise for variation if generating multiple samples\n                if i > 0:\n                    noise_level = 0.02 * i\n                    noise = torch.randn_like(sketch_tensor) * noise_level\n                    input_tensor = sketch_tensor + noise\n                else:\n                    input_tensor = sketch_tensor\n                \n                # Generate image\n                fake_image = G_sketch_to_real(input_tensor)\n                \n                # Convert to PIL image and save\n                fake_image = (fake_image.squeeze().cpu().detach() * 0.5 + 0.5).clamp(0, 1)\n                grid = torchvision.utils.make_grid(fake_image)\n                ndarr = grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n                im = Image.fromarray(ndarr)\n                \n                # Create output filename\n                base_name = os.path.splitext(os.path.basename(sketch_file))[0]\n                output_path = os.path.join(output_dir, f\"{base_name}_generated_{i+1}.png\")\n                im.save(output_path)\n                print(f\"Generated image saved to {output_path}\")\n\n# Main function to parse arguments and run training\nif __name__ == \"__main__\":\n    sketch_dir = \"/path/to/your/sketches\"  # Replace with your actual path\n    real_dir = \"/kaggle/input/dataset/photos\"  # Replace with your actual path\n    save_dir = \"./results\"\n\n    train_hybrid_gan(\n        sketch_dir=sketch_dir,\n        real_dir=real_dir,\n        epochs=100,\n        batch_size=8,\n        lr=0.0002,\n        beta1=0.5,\n        beta2=0.999,\n        lambda_cycle=10.0,\n        lambda_identity=5.0,\n        save_dir=save_dir,\n        sample_interval=100\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:29:08.924881Z","iopub.execute_input":"2025-03-10T13:29:08.925211Z","iopub.status.idle":"2025-03-10T13:50:22.047487Z","shell.execute_reply.started":"2025-03-10T13:29:08.925184Z","shell.execute_reply":"2025-03-10T13:50:22.046402Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"[Epoch 1/100] D_real: 0.1696, D_sketch: 0.0902, G: 8.6813, G_adv: 0.6761, G_cycle: 5.4464: 100%|██████████| 24/24 [00:11<00:00,  2.15it/s] \n[Epoch 2/100] D_real: 0.2162, D_sketch: 0.0708, G: 5.3106, G_adv: 0.7249, G_cycle: 3.2034: 100%|██████████| 24/24 [00:11<00:00,  2.14it/s]\n[Epoch 3/100] D_real: 0.1646, D_sketch: 0.1038, G: 5.0140, G_adv: 1.5511, G_cycle: 2.3711: 100%|██████████| 24/24 [00:11<00:00,  2.13it/s]\n[Epoch 4/100] D_real: 0.1811, D_sketch: 0.1862, G: 3.8090, G_adv: 0.5850, G_cycle: 2.1946: 100%|██████████| 24/24 [00:11<00:00,  2.11it/s]\n[Epoch 5/100] D_real: 0.2291, D_sketch: 0.0729, G: 4.1798, G_adv: 1.0746, G_cycle: 2.1504: 100%|██████████| 24/24 [00:11<00:00,  2.09it/s]\n[Epoch 6/100] D_real: 0.3239, D_sketch: 0.0988, G: 5.2127, G_adv: 1.7600, G_cycle: 2.5380: 100%|██████████| 24/24 [00:11<00:00,  2.07it/s]\n[Epoch 7/100] D_real: 0.1986, D_sketch: 0.1442, G: 4.2291, G_adv: 1.1091, G_cycle: 2.2481: 100%|██████████| 24/24 [00:11<00:00,  2.05it/s]\n[Epoch 8/100] D_real: 0.2089, D_sketch: 0.1106, G: 3.9103, G_adv: 1.0522, G_cycle: 2.0556: 100%|██████████| 24/24 [00:11<00:00,  2.04it/s]\n[Epoch 9/100] D_real: 0.2504, D_sketch: 0.1812, G: 3.1461, G_adv: 0.4421, G_cycle: 1.9259: 100%|██████████| 24/24 [00:11<00:00,  2.03it/s]\n[Epoch 10/100] D_real: 0.2145, D_sketch: 0.1394, G: 3.2840, G_adv: 0.4790, G_cycle: 2.0540: 100%|██████████| 24/24 [00:11<00:00,  2.02it/s]\n[Epoch 11/100] D_real: 0.2261, D_sketch: 0.2191, G: 3.1293, G_adv: 0.8033, G_cycle: 1.6427: 100%|██████████| 24/24 [00:11<00:00,  2.00it/s]\n[Epoch 12/100] D_real: 0.2302, D_sketch: 0.2065, G: 3.4915, G_adv: 0.9714, G_cycle: 1.7955: 100%|██████████| 24/24 [00:11<00:00,  2.00it/s]\n[Epoch 13/100] D_real: 0.2221, D_sketch: 0.1728, G: 3.0483, G_adv: 0.7283, G_cycle: 1.6616: 100%|██████████| 24/24 [00:12<00:00,  1.99it/s]\n[Epoch 14/100] D_real: 0.2330, D_sketch: 0.2483, G: 3.1733, G_adv: 0.5958, G_cycle: 1.8756: 100%|██████████| 24/24 [00:12<00:00,  1.99it/s]\n[Epoch 15/100] D_real: 0.1385, D_sketch: 0.1299, G: 2.9355, G_adv: 0.8283, G_cycle: 1.5051: 100%|██████████| 24/24 [00:12<00:00,  1.97it/s]\n[Epoch 16/100] D_real: 0.1391, D_sketch: 0.1419, G: 3.5532, G_adv: 0.6880, G_cycle: 2.1165: 100%|██████████| 24/24 [00:12<00:00,  1.97it/s]\n[Epoch 17/100] D_real: 0.3049, D_sketch: 0.2481, G: 2.4973, G_adv: 0.3630, G_cycle: 1.5707: 100%|██████████| 24/24 [00:12<00:00,  1.96it/s]\n[Epoch 18/100] D_real: 0.1817, D_sketch: 0.1560, G: 3.0097, G_adv: 0.8051, G_cycle: 1.5983: 100%|██████████| 24/24 [00:12<00:00,  1.96it/s]\n[Epoch 19/100] D_real: 0.1705, D_sketch: 0.2191, G: 2.3914, G_adv: 0.3858, G_cycle: 1.4554: 100%|██████████| 24/24 [00:12<00:00,  1.95it/s]\n[Epoch 20/100] D_real: 0.2343, D_sketch: 0.1945, G: 2.3373, G_adv: 0.5034, G_cycle: 1.3188: 100%|██████████| 24/24 [00:12<00:00,  1.95it/s]\n[Epoch 21/100] D_real: 0.1513, D_sketch: 0.0984, G: 3.2307, G_adv: 1.3231, G_cycle: 1.3809: 100%|██████████| 24/24 [00:12<00:00,  1.94it/s]\n[Epoch 22/100] D_real: 0.1618, D_sketch: 0.4657, G: 4.0640, G_adv: 1.8130, G_cycle: 1.6520: 100%|██████████| 24/24 [00:12<00:00,  1.95it/s]\n[Epoch 23/100] D_real: 0.2653, D_sketch: 0.0940, G: 2.1505, G_adv: 0.3912, G_cycle: 1.2916: 100%|██████████| 24/24 [00:12<00:00,  1.95it/s]\n[Epoch 24/100] D_real: 0.1556, D_sketch: 0.1466, G: 2.6026, G_adv: 0.6931, G_cycle: 1.3872: 100%|██████████| 24/24 [00:12<00:00,  1.94it/s]\n[Epoch 25/100] D_real: 0.2091, D_sketch: 0.1055, G: 2.3973, G_adv: 0.6045, G_cycle: 1.2987: 100%|██████████| 24/24 [00:12<00:00,  1.94it/s]\n[Epoch 26/100] D_real: 0.0968, D_sketch: 0.0226, G: 3.4909, G_adv: 1.1751, G_cycle: 1.7236: 100%|██████████| 24/24 [00:12<00:00,  1.93it/s]\n[Epoch 27/100] D_real: 0.1422, D_sketch: 0.0167, G: 3.3795, G_adv: 1.4843, G_cycle: 1.3772: 100%|██████████| 24/24 [00:12<00:00,  1.93it/s]\n[Epoch 28/100] D_real: 0.1550, D_sketch: 0.3130, G: 2.3121, G_adv: 0.3771, G_cycle: 1.4110: 100%|██████████| 24/24 [00:12<00:00,  1.93it/s]\n[Epoch 29/100] D_real: 0.1761, D_sketch: 0.2561, G: 2.5390, G_adv: 0.5439, G_cycle: 1.4327: 100%|██████████| 24/24 [00:12<00:00,  1.93it/s]\n[Epoch 30/100] D_real: 0.3695, D_sketch: 0.0606, G: 3.6337, G_adv: 1.3580, G_cycle: 1.6953: 100%|██████████| 24/24 [00:12<00:00,  1.93it/s]\n[Epoch 31/100] D_real: 0.1418, D_sketch: 0.0568, G: 3.5380, G_adv: 1.6677, G_cycle: 1.3539: 100%|██████████| 24/24 [00:12<00:00,  1.92it/s]\n[Epoch 32/100] D_real: 0.1474, D_sketch: 0.0299, G: 3.3856, G_adv: 1.3260, G_cycle: 1.5407: 100%|██████████| 24/24 [00:12<00:00,  1.92it/s]\n[Epoch 33/100] D_real: 0.0888, D_sketch: 0.0304, G: 3.5677, G_adv: 1.6262, G_cycle: 1.4319: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 34/100] D_real: 0.1163, D_sketch: 0.0314, G: 2.9171, G_adv: 1.0874, G_cycle: 1.3193: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 35/100] D_real: 0.0423, D_sketch: 0.1057, G: 3.5535, G_adv: 1.5277, G_cycle: 1.5063: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 36/100] D_real: 0.1048, D_sketch: 0.1614, G: 4.7415, G_adv: 2.7690, G_cycle: 1.4423: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 37/100] D_real: 0.1093, D_sketch: 0.0910, G: 3.7158, G_adv: 2.0827, G_cycle: 1.1927: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 38/100] D_real: 0.2117, D_sketch: 0.0085, G: 3.8346, G_adv: 2.1065, G_cycle: 1.2791: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 39/100] D_real: 0.0497, D_sketch: 0.0301, G: 2.9096, G_adv: 1.2681, G_cycle: 1.2192: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 40/100] D_real: 0.1138, D_sketch: 0.0372, G: 4.2574, G_adv: 2.3834, G_cycle: 1.3684: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 41/100] D_real: 0.0561, D_sketch: 0.0088, G: 3.7638, G_adv: 1.9906, G_cycle: 1.3032: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 42/100] D_real: 0.0569, D_sketch: 0.0079, G: 3.3760, G_adv: 1.7797, G_cycle: 1.1468: 100%|██████████| 24/24 [00:12<00:00,  1.92it/s]\n[Epoch 43/100] D_real: 0.2438, D_sketch: 0.0111, G: 3.0572, G_adv: 1.2724, G_cycle: 1.2837: 100%|██████████| 24/24 [00:12<00:00,  1.93it/s]\n[Epoch 44/100] D_real: 0.1089, D_sketch: 0.0178, G: 3.1958, G_adv: 1.5427, G_cycle: 1.2004: 100%|██████████| 24/24 [00:12<00:00,  1.92it/s]\n[Epoch 45/100] D_real: 0.0589, D_sketch: 0.0038, G: 3.3302, G_adv: 1.7428, G_cycle: 1.1259: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 46/100] D_real: 0.1079, D_sketch: 0.0151, G: 4.2567, G_adv: 2.5717, G_cycle: 1.2295: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 47/100] D_real: 0.0225, D_sketch: 0.0028, G: 3.5127, G_adv: 1.9041, G_cycle: 1.1711: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 48/100] D_real: 0.0213, D_sketch: 0.0066, G: 3.6544, G_adv: 1.7986, G_cycle: 1.3927: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 49/100] D_real: 0.0692, D_sketch: 0.0767, G: 2.3603, G_adv: 0.8119, G_cycle: 1.1195: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 50/100] D_real: 0.0425, D_sketch: 0.1224, G: 2.3673, G_adv: 0.9089, G_cycle: 1.0656: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 51/100] D_real: 0.0391, D_sketch: 0.1328, G: 2.4767, G_adv: 0.8221, G_cycle: 1.2070: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 52/100] D_real: 0.0249, D_sketch: 0.1453, G: 2.9326, G_adv: 1.2839, G_cycle: 1.2169: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 53/100] D_real: 0.6299, D_sketch: 0.0462, G: 3.4274, G_adv: 1.8808, G_cycle: 1.1390: 100%|██████████| 24/24 [00:12<00:00,  1.92it/s]\n[Epoch 54/100] D_real: 0.0170, D_sketch: 0.0075, G: 3.4171, G_adv: 1.7201, G_cycle: 1.2356: 100%|██████████| 24/24 [00:12<00:00,  1.92it/s]\n[Epoch 55/100] D_real: 0.0579, D_sketch: 0.0060, G: 4.1401, G_adv: 2.6254, G_cycle: 1.1040: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 56/100] D_real: 0.0813, D_sketch: 0.0424, G: 3.0519, G_adv: 1.4880, G_cycle: 1.1337: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 57/100] D_real: 0.0148, D_sketch: 0.0104, G: 3.5650, G_adv: 2.0666, G_cycle: 1.0868: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 58/100] D_real: 0.0207, D_sketch: 0.0481, G: 2.6169, G_adv: 1.2080, G_cycle: 1.0369: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 59/100] D_real: 0.0359, D_sketch: 0.0143, G: 2.5878, G_adv: 1.2118, G_cycle: 1.0037: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 60/100] D_real: 0.0050, D_sketch: 0.0605, G: 2.9274, G_adv: 1.5721, G_cycle: 0.9808: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 61/100] D_real: 0.0187, D_sketch: 0.0058, G: 3.4069, G_adv: 1.8180, G_cycle: 1.1458: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 62/100] D_real: 0.0860, D_sketch: 0.0019, G: 3.4069, G_adv: 1.9921, G_cycle: 1.0276: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 63/100] D_real: 0.1164, D_sketch: 0.0036, G: 2.7977, G_adv: 1.2848, G_cycle: 1.1105: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 64/100] D_real: 0.0462, D_sketch: 0.0023, G: 2.6860, G_adv: 1.4670, G_cycle: 0.8868: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 65/100] D_real: 0.0317, D_sketch: 0.0031, G: 2.9124, G_adv: 1.4652, G_cycle: 1.0691: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 66/100] D_real: 0.0626, D_sketch: 0.0025, G: 2.6552, G_adv: 1.3678, G_cycle: 0.9310: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 67/100] D_real: 0.0219, D_sketch: 0.0010, G: 3.0015, G_adv: 1.6767, G_cycle: 0.9540: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 68/100] D_real: 0.0395, D_sketch: 0.0020, G: 2.8194, G_adv: 1.3697, G_cycle: 1.0545: 100%|██████████| 24/24 [00:12<00:00,  1.89it/s]\n[Epoch 69/100] D_real: 0.0080, D_sketch: 0.0018, G: 3.4846, G_adv: 1.9659, G_cycle: 1.0978: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 70/100] D_real: 0.0176, D_sketch: 0.0016, G: 3.4619, G_adv: 2.1194, G_cycle: 0.9748: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 71/100] D_real: 0.0280, D_sketch: 0.0015, G: 3.0365, G_adv: 1.6430, G_cycle: 1.0088: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 72/100] D_real: 0.0334, D_sketch: 0.0017, G: 2.7949, G_adv: 1.5332, G_cycle: 0.9192: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 73/100] D_real: 0.0077, D_sketch: 0.0008, G: 3.2532, G_adv: 1.9536, G_cycle: 0.9568: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 74/100] D_real: 0.0060, D_sketch: 0.0027, G: 3.1208, G_adv: 1.9104, G_cycle: 0.8821: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 75/100] D_real: 0.0069, D_sketch: 0.0008, G: 3.3454, G_adv: 2.0426, G_cycle: 0.9582: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 76/100] D_real: 0.0023, D_sketch: 0.0026, G: 3.4555, G_adv: 2.0193, G_cycle: 1.0547: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 77/100] D_real: 0.0082, D_sketch: 0.0051, G: 3.1504, G_adv: 1.8632, G_cycle: 0.9554: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 78/100] D_real: 0.0108, D_sketch: 0.0013, G: 3.4662, G_adv: 2.1496, G_cycle: 0.9582: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 79/100] D_real: 0.0333, D_sketch: 0.0010, G: 3.6135, G_adv: 2.2822, G_cycle: 0.9665: 100%|██████████| 24/24 [00:12<00:00,  1.91it/s]\n[Epoch 80/100] D_real: 0.0376, D_sketch: 0.0012, G: 3.1292, G_adv: 1.8960, G_cycle: 0.8929: 100%|██████████| 24/24 [00:12<00:00,  1.90it/s]\n[Epoch 81/100] D_real: 0.0080, D_sketch: 0.0101, G: 2.9015, G_adv: 1.6911, G_cycle: 0.8898: 100%|██████████| 24/24 [00:12<00:00,  1.88it/s]\n[Epoch 82/100] D_real: 0.0569, D_sketch: 0.0249, G: 2.6945, G_adv: 1.3538, G_cycle: 0.9858: 100%|██████████| 24/24 [00:12<00:00,  1.88it/s]\n[Epoch 83/100] D_real: 0.0237, D_sketch: 0.0084, G: 2.9410, G_adv: 1.7946, G_cycle: 0.8362: 100%|██████████| 24/24 [00:12<00:00,  1.88it/s]\n[Epoch 84/100] D_real: 0.0071, D_sketch: 0.0013, G: 3.0919, G_adv: 1.8727, G_cycle: 0.8864: 100%|██████████| 24/24 [00:12<00:00,  1.88it/s]\n[Epoch 85/100] D_real: 0.0076, D_sketch: 0.0015, G: 3.0985, G_adv: 1.9698, G_cycle: 0.8119: 100%|██████████| 24/24 [00:12<00:00,  1.88it/s]\n[Epoch 86/100] D_real: 0.0086, D_sketch: 0.0178, G: 3.1835, G_adv: 1.9870, G_cycle: 0.8737: 100%|██████████| 24/24 [00:12<00:00,  1.86it/s]\n[Epoch 87/100] D_real: 0.0053, D_sketch: 0.0011, G: 2.9653, G_adv: 1.8780, G_cycle: 0.7930: 100%|██████████| 24/24 [00:13<00:00,  1.84it/s]\n[Epoch 88/100] D_real: 0.0135, D_sketch: 0.0073, G: 3.1891, G_adv: 1.9839, G_cycle: 0.8586: 100%|██████████| 24/24 [00:12<00:00,  1.85it/s]\n[Epoch 89/100] D_real: 0.0221, D_sketch: 0.0139, G: 2.8482, G_adv: 1.5727, G_cycle: 0.9302: 100%|██████████| 24/24 [00:12<00:00,  1.85it/s]\n[Epoch 90/100] D_real: 0.0252, D_sketch: 0.0091, G: 3.3915, G_adv: 2.1830, G_cycle: 0.8654: 100%|██████████| 24/24 [00:12<00:00,  1.86it/s]\n[Epoch 91/100] D_real: 0.0299, D_sketch: 0.1121, G: 2.0816, G_adv: 0.8851, G_cycle: 0.8654: 100%|██████████| 24/24 [00:12<00:00,  1.86it/s]\n[Epoch 92/100] D_real: 0.0543, D_sketch: 0.0020, G: 2.6906, G_adv: 1.4757, G_cycle: 0.8544: 100%|██████████| 24/24 [00:12<00:00,  1.86it/s]\n[Epoch 93/100] D_real: 0.0211, D_sketch: 0.0612, G: 2.7339, G_adv: 1.5253, G_cycle: 0.8728: 100%|██████████| 24/24 [00:12<00:00,  1.85it/s]\n[Epoch 94/100] D_real: 0.0041, D_sketch: 0.0610, G: 2.8449, G_adv: 1.6690, G_cycle: 0.8513: 100%|██████████| 24/24 [00:12<00:00,  1.85it/s]\n[Epoch 95/100] D_real: 0.0233, D_sketch: 0.1050, G: 2.3013, G_adv: 1.0799, G_cycle: 0.8890: 100%|██████████| 24/24 [00:13<00:00,  1.84it/s]\n[Epoch 96/100] D_real: 0.0776, D_sketch: 0.0337, G: 2.4198, G_adv: 1.2202, G_cycle: 0.8683: 100%|██████████| 24/24 [00:13<00:00,  1.84it/s]\n[Epoch 97/100] D_real: 0.1893, D_sketch: 0.0756, G: 1.8817, G_adv: 0.8065, G_cycle: 0.7826: 100%|██████████| 24/24 [00:12<00:00,  1.85it/s]\n[Epoch 98/100] D_real: 0.1320, D_sketch: 0.0790, G: 2.3318, G_adv: 1.1724, G_cycle: 0.8387: 100%|██████████| 24/24 [00:12<00:00,  1.87it/s]\n[Epoch 99/100] D_real: 0.1178, D_sketch: 0.1269, G: 2.0273, G_adv: 0.8144, G_cycle: 0.8721: 100%|██████████| 24/24 [00:12<00:00,  1.86it/s]\n[Epoch 100/100] D_real: 0.1896, D_sketch: 0.1745, G: 1.7513, G_adv: 0.7369, G_cycle: 0.7321: 100%|██████████| 24/24 [00:12<00:00,  1.85it/s]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!find . -type f | sort\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:56:32.738459Z","iopub.execute_input":"2025-03-10T13:56:32.738786Z","iopub.status.idle":"2025-03-10T13:56:32.968167Z","shell.execute_reply.started":"2025-03-10T13:56:32.738763Z","shell.execute_reply":"2025-03-10T13:56:32.967181Z"}},"outputs":[{"name":"stdout","text":"./results/checkpoints/model_epoch_100.pth\n./results/checkpoints/model_epoch_10.pth\n./results/checkpoints/model_epoch_20.pth\n./results/checkpoints/model_epoch_30.pth\n./results/checkpoints/model_epoch_40.pth\n./results/checkpoints/model_epoch_50.pth\n./results/checkpoints/model_epoch_60.pth\n./results/checkpoints/model_epoch_70.pth\n./results/checkpoints/model_epoch_80.pth\n./results/checkpoints/model_epoch_90.pth\n./results/samples/epoch_100_batch_0.png\n./results/samples/epoch_10_batch_0.png\n./results/samples/epoch_11_batch_0.png\n./results/samples/epoch_12_batch_0.png\n./results/samples/epoch_13_batch_0.png\n./results/samples/epoch_14_batch_0.png\n./results/samples/epoch_15_batch_0.png\n./results/samples/epoch_16_batch_0.png\n./results/samples/epoch_17_batch_0.png\n./results/samples/epoch_18_batch_0.png\n./results/samples/epoch_19_batch_0.png\n./results/samples/epoch_1_batch_0.png\n./results/samples/epoch_20_batch_0.png\n./results/samples/epoch_21_batch_0.png\n./results/samples/epoch_22_batch_0.png\n./results/samples/epoch_23_batch_0.png\n./results/samples/epoch_24_batch_0.png\n./results/samples/epoch_25_batch_0.png\n./results/samples/epoch_26_batch_0.png\n./results/samples/epoch_27_batch_0.png\n./results/samples/epoch_28_batch_0.png\n./results/samples/epoch_29_batch_0.png\n./results/samples/epoch_2_batch_0.png\n./results/samples/epoch_30_batch_0.png\n./results/samples/epoch_31_batch_0.png\n./results/samples/epoch_32_batch_0.png\n./results/samples/epoch_33_batch_0.png\n./results/samples/epoch_34_batch_0.png\n./results/samples/epoch_35_batch_0.png\n./results/samples/epoch_36_batch_0.png\n./results/samples/epoch_37_batch_0.png\n./results/samples/epoch_38_batch_0.png\n./results/samples/epoch_39_batch_0.png\n./results/samples/epoch_3_batch_0.png\n./results/samples/epoch_40_batch_0.png\n./results/samples/epoch_41_batch_0.png\n./results/samples/epoch_42_batch_0.png\n./results/samples/epoch_43_batch_0.png\n./results/samples/epoch_44_batch_0.png\n./results/samples/epoch_45_batch_0.png\n./results/samples/epoch_46_batch_0.png\n./results/samples/epoch_47_batch_0.png\n./results/samples/epoch_48_batch_0.png\n./results/samples/epoch_49_batch_0.png\n./results/samples/epoch_4_batch_0.png\n./results/samples/epoch_50_batch_0.png\n./results/samples/epoch_51_batch_0.png\n./results/samples/epoch_52_batch_0.png\n./results/samples/epoch_53_batch_0.png\n./results/samples/epoch_54_batch_0.png\n./results/samples/epoch_55_batch_0.png\n./results/samples/epoch_56_batch_0.png\n./results/samples/epoch_57_batch_0.png\n./results/samples/epoch_58_batch_0.png\n./results/samples/epoch_59_batch_0.png\n./results/samples/epoch_5_batch_0.png\n./results/samples/epoch_60_batch_0.png\n./results/samples/epoch_61_batch_0.png\n./results/samples/epoch_62_batch_0.png\n./results/samples/epoch_63_batch_0.png\n./results/samples/epoch_64_batch_0.png\n./results/samples/epoch_65_batch_0.png\n./results/samples/epoch_66_batch_0.png\n./results/samples/epoch_67_batch_0.png\n./results/samples/epoch_68_batch_0.png\n./results/samples/epoch_69_batch_0.png\n./results/samples/epoch_6_batch_0.png\n./results/samples/epoch_70_batch_0.png\n./results/samples/epoch_71_batch_0.png\n./results/samples/epoch_72_batch_0.png\n./results/samples/epoch_73_batch_0.png\n./results/samples/epoch_74_batch_0.png\n./results/samples/epoch_75_batch_0.png\n./results/samples/epoch_76_batch_0.png\n./results/samples/epoch_77_batch_0.png\n./results/samples/epoch_78_batch_0.png\n./results/samples/epoch_79_batch_0.png\n./results/samples/epoch_7_batch_0.png\n./results/samples/epoch_80_batch_0.png\n./results/samples/epoch_81_batch_0.png\n./results/samples/epoch_82_batch_0.png\n./results/samples/epoch_83_batch_0.png\n./results/samples/epoch_84_batch_0.png\n./results/samples/epoch_85_batch_0.png\n./results/samples/epoch_86_batch_0.png\n./results/samples/epoch_87_batch_0.png\n./results/samples/epoch_88_batch_0.png\n./results/samples/epoch_89_batch_0.png\n./results/samples/epoch_8_batch_0.png\n./results/samples/epoch_90_batch_0.png\n./results/samples/epoch_91_batch_0.png\n./results/samples/epoch_92_batch_0.png\n./results/samples/epoch_93_batch_0.png\n./results/samples/epoch_94_batch_0.png\n./results/samples/epoch_95_batch_0.png\n./results/samples/epoch_96_batch_0.png\n./results/samples/epoch_97_batch_0.png\n./results/samples/epoch_98_batch_0.png\n./results/samples/epoch_99_batch_0.png\n./results/samples/epoch_9_batch_0.png\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!zip model_files.zip ./results/checkpoints/model_epoch_100.pth\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T14:05:34.905393Z","iopub.execute_input":"2025-03-10T14:05:34.905753Z","iopub.status.idle":"2025-03-10T14:06:17.780049Z","shell.execute_reply.started":"2025-03-10T14:05:34.905723Z","shell.execute_reply":"2025-03-10T14:06:17.778982Z"}},"outputs":[{"name":"stdout","text":"  adding: results/checkpoints/model_epoch_100.pth (deflated 10%)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!ls -lh /kaggle/working/model_files.zip\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T14:11:57.245987Z","iopub.execute_input":"2025-03-10T14:11:57.246356Z","iopub.status.idle":"2025-03-10T14:11:57.387122Z","shell.execute_reply.started":"2025-03-10T14:11:57.246323Z","shell.execute_reply":"2025-03-10T14:11:57.386057Z"}},"outputs":[{"name":"stdout","text":"-rw-r--r-- 1 root root 673M Mar 10 14:06 /kaggle/working/model_files.zip\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}